<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Summary Statistics</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
  body {
    padding: 2cm; 
  }
}
</style>

<style type="text/css">
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
	color: black;
	background: none;
	text-shadow: 0 1px white;
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
	text-shadow: none;
	background: #b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
	text-shadow: none;
	background: #b3d4fc;
}

@media print {
	code[class*="language-"],
	pre[class*="language-"] {
		text-shadow: none;
	}
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #f5f2f0;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: slategray;
}

.token.punctuation {
	color: #999;
}

.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
	color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
	color: #a67f59;
	background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
	color: #07a;
}

.token.function {
	color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
	color: #e90;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
</style>


</head>

<body>

<h1 id="toc_0">Applied statistics - Summary &amp; code</h1>

<p><br><br>
<br><br>
<br>
<br></p>

<p align="center">
  <img width="500" height="350" src="https://miro.medium.com/max/3840/1*UAGU532MbhR5cm3symwWqg.png">
</p>  

<p><br><br>
<br><br>
<p align="left"> <img width="200" height="80" src="BSM_logo.png">
<br>
<em>Prepared for exam and review by Daniëlle Kotter</em></p>

<div style="page-break-after: always;"></div>

<h1 id="toc_1">Table of content</h1>

<ul class="toc">
<li>
<a href="#toc_0">Applied statistics - Summary &amp; code</a>
</li>
<li>
<a href="#toc_1">Table of content</a>
<ul>
<li>
<a href="#toc_2">Basics</a>
<ul>
<li>
<a href="#toc_3">Basic commands:</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">Topic 1 - Probability &amp; Statistical inference</a>
<ul>
<li>
<a href="#toc_5">Bayes Theorem</a>
</li>
<li>
<a href="#toc_6">Statistical inference:</a>
</li>
<li>
<a href="#toc_7">Sample:</a>
</li>
<li>
<a href="#toc_8">How to choose a sample size:</a>
</li>
</ul>
</li>
<li>
<a href="#toc_9">Topic 2 - Discrete probability</a>
<ul>
<li>
<a href="#toc_10">Uniform discrete probability distribution</a>
</li>
<li>
<a href="#toc_11">Binomial distribution</a>
</li>
<li>
<a href="#toc_12">Poisson distribution</a>
</li>
<li>
<a href="#toc_13">Examples different distributions:</a>
</li>
</ul>
</li>
<li>
<a href="#toc_14">Topic 3 - The normal distribution</a>
<ul>
<li>
<a href="#toc_15">Plotting the normal distribution</a>
</li>
<li>
<a href="#toc_16">Binomial</a>
</li>
<li>
<a href="#toc_17">Poisson</a>
</li>
</ul>
</li>
<li>
<a href="#toc_18">Topic 4 - Samples, estimation &amp; confidence intervals</a>
</li>
<li>
<a href="#toc_19">Topic 5 - Significance testing</a>
<ul>
<li>
<a href="#toc_20">Hypothesis testing</a>
</li>
<li>
<a href="#toc_21">The difference between confidence interval and a test of significance</a>
</li>
<li>
<a href="#toc_22">Critical values</a>
</li>
<li>
<a href="#toc_23">Type of errors:</a>
</li>
<li>
<a href="#toc_24">Sample significance testing</a>
</li>
<li>
<a href="#toc_25">Test of equality</a>
</li>
<li>
<a href="#toc_26">P-value</a>
</li>
</ul>
</li>
<li>
<a href="#toc_27">Topic 5 - Non-Parametric testing</a>
<ul>
<li>
<a href="#toc_28">Chi-square</a>
</li>
<li>
<a href="#toc_29">Goodness of fit</a>
</li>
<li>
<a href="#toc_30">Normal distribution</a>
</li>
<li>
<a href="#toc_31">Mann-whitney test</a>
</li>
<li>
<a href="#toc_32">Wilcoxon test</a>
</li>
<li>
<a href="#toc_33">Run test</a>
</li>
<li>
<a href="#toc_34">P-value</a>
</li>
</ul>
</li>
<li>
<a href="#toc_35">Topic 6 - Regressions, correlation and dummy&#39;s</a>
<ul>
<li>
<a href="#toc_36">R-Squared</a>
</li>
<li>
<a href="#toc_37">Regressions</a>
</li>
<li>
<a href="#toc_38">Dummy variables, diff in means</a>
</li>
<li>
<a href="#toc_39">Comparing regressions</a>
</li>
</ul>
</li>
<li>
<a href="#toc_40">Topic 7 - Prediction</a>
<ul>
<li>
<a href="#toc_41">Predictions</a>
</li>
<li>
<a href="#toc_42">Prediction with dummy variables</a>
</li>
<li>
<a href="#toc_43">Prediction intervals example with dummies</a>
</li>
</ul>
</li>
<li>
<a href="#toc_44">Topic 8 - Data problems</a>
<ul>
<li>
<a href="#toc_45">Multicollinearity</a>
</li>
<li>
<a href="#toc_46">ANOVA: Analysis of variance test.</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<div style="page-break-after: always;"></div>

<hr>

<h2 id="toc_2">Basics</h2>

<p><strong>Statistics:</strong> the science of learning from data<br>
<strong>Individuals:</strong> the objects described in a set of data -&gt; cases </p>

<p><strong>Population:</strong> individuals / object we want to study<br>
<strong>Variable:</strong> any characteristic of an individual. There are 4 types of variables:</p>

<p><strong>- Qualitative:</strong> Classifies data categorical<br>
        ○ <strong>Nominal:</strong> Not ordered. F.E. gender<br>
        ○ <strong>Ordinal:</strong> Ordered. F.E. Age group<br>
<strong>- Quantitative:</strong> Measures between data<br>
        ○ <strong>Interval:</strong> No real 0. F.E. Temp in degrees<br>
        ○ <strong>Ratio:</strong> Absolute 0. F.E. Salary  </p>

<p><strong>Primary data:</strong> information that is not available and needs to be collected ourselves.<br>
<strong>Secondary data:</strong> information already available through research of others.</p>

<p><strong>Census:</strong> full list of population. Very unlikely to be collected due to scale.<br>
<strong>Sample:</strong> subgroup from the wider population.</p>

<p><strong>Descriptive statistics:</strong> identify characteristics of data sample group.<br>
<strong>Inferential statistics:</strong> infer data from subgroup to the entire population.</p>

<p><strong>Longitudinal:</strong> aiming to forecast for a time frame.<br>
<strong>Cross-sectional:</strong> in a particular moment of time. </p>

<h3 id="toc_3">Basic commands:</h3>

<p><strong>Basic packages:</strong></p>

<div><pre><code class="language-none">  &quot;prob&quot;,
  &quot;data.table&quot;,
  &quot;distrEx&quot;,
  &quot;LaplacesDemon&quot;,
  &quot;formattable&quot;,
  &quot;kableExtra&quot;,
  &quot;knitr&quot;,
  &quot;TeachingDemos&quot;,
  &quot;dplyr&quot;,
  &quot;dbplyr&quot;,
  &quot;tidyverse&quot;,
  &quot;Hmisc&quot;,
  &quot;psych&quot;,
  &quot;samplingbook&quot;,
  &quot;swirl&quot;,
  &quot;ggplot2&quot;,
  &quot;swirl&quot;,
  &quot;snpar&quot;,
  &quot;BSDA&quot;,
  &quot;actuar&quot;,
  &quot;readxl&quot;,
  &quot;stargazer&quot;  </code></pre></div>

<p><strong>Sample mean, standard deviation</strong></p>

<div><pre><code class="language-none">mean(variable)
sd(variable)</code></pre></div>

<p>Removes values NA in a data set:</p>

<div><pre><code class="language-none">mean(variable, na.rm = TRUE)
sd(variable, na.rm = TRUE)</code></pre></div>

<p><strong>Weighted mean &amp; standard deviation</strong></p>

<p>Package: &quot;HMISC&quot;.</p>

<div><pre><code class="language-none">weightedmean &lt;- Xbar = wtd.mean(x,y)
weightedsd &lt;- SQRT(Wtd.var(X,Y))/sqrt(n)</code></pre></div>

<p><strong>Variance</strong></p>

<div><pre><code class="language-none">var(x)</code></pre></div>

<p><strong>Tables frames &amp; Matrixes</strong></p>

<div><pre><code class="language-none">matrix(c(1,2,3,4,5,6,7,8), nrow = 4, byrow = TRUE) &lt;- organized by row
matrix(c(1,2,3,4,5,6,7,8), ncol = 4, byrow = FALSE) &lt;- organized by col
data.frame(Column1 = c(1,2,3,4,5), Column2 = c(1,2,3,4,5))
data.table(Column1 = c(1,2,3,4,5), Column2 = c(1,2,3,4,5))

as.table(matrix(c(1,2,3,4,5,6,7,8), nrow = 4))</code></pre></div>

<div><pre><code class="language-none">rbind(data, newvariable)
cbind(data, newvariable)

rownames(datatable) &lt;- c()
colnames(datatable) &lt;- c()</code></pre></div>

<p><strong>Other:</strong></p>

<div><pre><code class="language-none">round(value, 2) &lt;- two decimals
as.numeric(value)
rep(5,5) &lt;- repeats 5, 5 times
percent(value) &lt;- presents 0.25 as -&gt; 25.00%
describe(variable)
fivenum(variable)
summary(variable)
str(variable) &lt;- explains the variable</code></pre></div>

<p><strong>Read excel</strong></p>

<div><pre><code class="language-none">library(readxl)
data &lt;- read.xls(&quot;data.xlsx&quot;, stringsAsFactors = TRUE)</code></pre></div>

<p><strong>Mathmatical values</strong></p>

<div><pre><code class="language-none">$\mu$ &lt;- Population mean   
$\sigma$ &lt;- Population sd   
$\bar{x}$ &lt;- Sample mean   
${e}$ &lt;- Standard error  
$\pi$ &lt;- pie
$\ge$ &lt;- Bigger than     
$\le$ &lt;- Smaller than  </code></pre></div>

<div style="page-break-after: always;"></div>

<h2 id="toc_4">Topic 1 - Probability &amp; Statistical inference</h2>

<p><strong>Experiment:</strong> trial, action that leads to one or several outcomes.<br>
<strong>Sample space:</strong> list of all possible mutually exclusive outcomes. Sum of prob: is 1.<br>
<strong>Event:</strong> one outcome or combined outcomes. F.E. Walking the dog / women walking the dog.<br>
<strong>Expected outcome:</strong> Probability of that particular outcome on a single trial * Total number of trials.  </p>

<p><strong>Axioms of probability</strong><br>
    - Prob always between 0 and 1<br>
    - Certain occurrence prob = 1<br>
    - Sample space = sum of prob = 1<br>
    - Prob of two mutual events = sum of their prob<br>
    - Prob of event not occurring -&gt; compliment = 1 - its probability of it occurring  </p>

<p><strong>Basic relationships:</strong><br>
<strong>1. Mutually exclusive event:</strong> assign probabilities to outcomes that cannot occur at the same time.<br>
<strong>2. Non-mutually exclusive event:</strong> The probabilities of events can be summed. There is a possibility that events happen together.<br>
<strong>3. Independent events:</strong> prob event * prob event2. F.E. 2 heads in a row<br>
<strong>4. Dependent events:</strong> probabilities of the events are conditional. F.E. Pricking out a group of women/male. Once you choose a male, there is one male less to choose.  </p>

<p><strong>Subjective definition</strong><br>
Some subjects are able to assess the probability of an event before it occurs (Examples: local weather, production line knowledge, management outcomes). Survey methods to construct this subjective probabilities and then use it for the analysis. </p>

<p><strong>Expected values:</strong> If we know probabilities and values we can work out Expected Values (EV), sometimes referred to as Expected Monetary Values (EMV) by multiplying the value of the outcomes by probabilities.</p>

<p><p align="center"> <img width="350" height="300" src="prob.png"></p>

<p><strong>Probability space</strong></p>

<p>Package = &quot;prob&quot;.</p>

<div><pre><code class="language-none">out &lt;- c(“Red”,“White”,“Black”,“Blue”,“Green”)
freq &lt;- c(1,2,3,4,5)
s &lt;- probspace(out,probs=freq)</code></pre></div>

<p><em>If you toss two fair coins, what is the probability of two heads?</em></p>

<div><pre><code class="language-none">space &lt;- tosscoin(2,makespace=TRUE)
p &lt;- Prob(space, toss1==&quot;H&quot; &amp; toss2==&quot;H&quot;)</code></pre></div>

<p><em>When two dice are thrown, what is the probability of a 3 followed by a 5?</em></p>

<div><pre><code class="language-none">space &lt;- rolldie(2, makespace = TRUE)
p &lt;- Prob(space, X1 == 3 &amp; (X2 == 5) )</code></pre></div>

<p><em>Sampling from an urn with or without replacement. 3 balls and sample size of 2:</em></p>

<div><pre><code class="language-none">urnsamples(1:3, size = 2, replace = TRUE, ordered = TRUE)
urnsamples(1:3, size = 2, replace = FALSE, ordered = TRUE)
urnsamples(1:3, size = 2, replace = FALSE, ordered = FALSE)
urnsamples(1:3, size = 2, replace = TRUE, ordered = FALSE)</code></pre></div>

<h3 id="toc_5">Bayes Theorem</h3>

<p><strong>Posterior probabilities:</strong> the probability an event will happen after all evidence or background information has been taken into account. F.E. Chance that an item is effective. </p>

<p><strong>Prior probability:</strong> the probability an event will happen before you taken any new evidence into account. This information influences your prediction of probability. F.E. 40% of product comes from machine A.</p>

<p><strong>Conditional probability:</strong> the probability of one event occurring with some relationship to one or more other events. F.E. to calculate whether a defect item comes from machine A.
P(Machine A, Defective) / P(Defective).  </p>

<p><p align="center"> <img width="350" height="250" src="bayes.png"></p>

<p><strong>Unconditional probability:</strong></p>

<p>P(S) and P(NS)
Success or no success</p>

<div><pre><code class="language-none">prS &lt;- c(0.4, 0.6)</code></pre></div>

<p><strong>Conditional probability:</strong></p>

<p>P(P | S ) and P( P | NS)
Predicted given it is successful<br>
Predicted given it is not successful</p>

<div><pre><code class="language-none">prNS &lt;- c(0.6, 0.2)</code></pre></div>

<p><strong>Bayes prob</strong><br>
Posterior probabilities<br>
P(S | P) &amp; P(NS | P)</p>

<div><pre><code class="language-none">BayesTheorem(prS, prNS)</code></pre></div>

<h3 id="toc_6">Statistical inference:</h3>

<p><strong>Inferential statistics:</strong> derives properties and conclusions of a population based on the information of a sample. </p>

<p><em>Providing a</em><br>
    - Statement expressed in probability<br>
    - Purpose of drawing conclusions from data<br>
    - Act as if the data comes from a random sample<br>
    - Must have the unrealistic assumption that standard deviation of the population is known  </p>

<p><strong>The goodness of result will depend upon:</strong><br>
- The size of the sample!<br>
- The variability of the sample!<br>
- Level of confidence!  </p>

<p><strong>Unbiased:</strong> there is no systematic tendency to underestimate or over estimate the truth. The following statistics are unbiased:<br>
\(\mu\) = Population mean<br>
\(\sigma\) = Population standard deviation<br>
\(\bar{x}\) = Sample mean<br>
s = Sample standard deviation  </p>

<p><strong>Confidence interval</strong> = Confidence level that shows the probability of producing an interval that contains the unknown parameter. Interval of numbers between mean +/- margin of error.</p>

<p><strong>Margin of error:</strong> Reflects how accurate we believe the estimate is based on variability &amp; confidence. It is a procedure to catch the true population mean.<br>
Margin of error = z * sd / sqrt(n)</p>

<p><strong>What to do when the margin of error is too large:</strong><br>
    - Use lower level of confidence<br>
    - Increase sample size<br>
    - Reduce standard deviation. F.E. Controlling measurement process or changing population sample  </p>

<h3 id="toc_7">Sample:</h3>

<p>Subgroups of a population used to draw inferences about characteristics of population. Sample must be representative of the population. We know that the value of the sample mean will vary from sample to sample – follow a sampling distribution: We would expect the sample means to vary around the true
(population) mean. The variation will depend on sample size, the larger the
sample the less spread the (sampling) distribution</p>

<p><strong>Parameter</strong> = characteristic that describes the population<br>
<strong>Statistics</strong> = characteristic that describes a sample. Used to estimate the unknown parameter.   </p>

<p><strong>Aspects to take into account while sampling:</strong><br>
    1. Definition of the objective population<br>
    2. Determine base for sampling<br>
    3. Determine sampling method<br>
    4. Choose margin of error / sample size<br>
    5. Sampling execution  </p>

<p><strong>Non probabilistic sampling:</strong> convenience, voluntary responses<br>
<strong>Probabilistic sampling:</strong> random &amp; satisfied. All individuals have the same probability in being included in the sampling.</p>

<p><strong>Non probabilistic methods:</strong><br>
    <strong>1. Convenience sampling:</strong> asking whoever is easily available. Convenient &amp; cheap. Risk of biased.<br>
    <strong>2. Voluntary response sampling:</strong> answer if you want. Response already biased and who you reach also biased.<br>
    <strong>3. Quota sampling:</strong> choose sample based on characteristics that represent the population. F.E. 50% men/female  </p>

<p><strong>Probabilistic methods:</strong><br>
    <strong>1. Simple random sample:</strong> actually random (pulling number or randomizers). Required full list of the population.<br>
    <strong>2. Stratified random sample:</strong> identify relevant characteristics. Randomly sampled from each proportion characteristics.<br>
    <strong>3. Joint stratified random sample:</strong> size depends on relative weight due to several characteristics. Demanding and large scale techniques.<br>
    <strong>4. Non-proportional stratified sampling:</strong> over representing minorities to capture true heterogeneity of opinions.  </p>

<h3 id="toc_8">How to choose a sample size:</h3>

<p><strong>Statistically:</strong><br>
    - Variability in the variable within the population<br>
    - How confident do we want to be in a precise estimate  </p>

<p><strong>Economically</strong><br>
    - Cost of obtaining the sample<br>
    - Worth cost/benefit in decision making   </p>

<p><strong>Desirables for estimators:</strong><br>
    - Unbiased: expected value of the statistical equals parameters<br>
    - Consistency: bias decrease when sample size increases<br>
    - Efficiency: unbiased estimator with the smallest variance is the most efficiet estimator.  </p>

<div style="page-break-after: always;"></div>

<h2 id="toc_9">Topic 2 - Discrete probability</h2>

<h3 id="toc_10">Uniform discrete probability distribution</h3>

<p>This distribution describes situations where all outcomes are equally likely; it is the assumption you make about a “fair coin”, or a pack of cards.</p>

<p>If we generalize this situation we might define a discrete uniform distribution as one where there are a given number of possible outcomes, and each has the same probability. Putting this another way, if there are n outcomes, then the probability of any particular outcome will be 1/n. </p>

<p>Throwing two dies and adding up the amounts is not uniform distribution. There are multiple ways of getting a 7 for example. </p>

<ol>
<li>Sample space with a set probability. Size = amount of tries</li>
<li>Density function: Individual probability. F.E. Getting a 4</li>
<li>Cumulatative density: Uniform for a certain value distribution. F.E. 4 or less. 4 or more? 1-punif 3</li>
<li>Inverse cumulative density: Uniform for a certain probability ( up until a certain value). F.E. up to 25% of the tries</li>
</ol>

<div><pre><code class="language-none">1. sample(p, size=n, replace=TRUE)
2. dunif(X, min = a, max = b)
3. punif(X, min=0, max=6)
4. qunif(X, min=0, max=6)</code></pre></div>

<p><strong><em>Default = # or less. For # or more do: 1-probability of # or less</em></strong> </p>

<h3 id="toc_11">Binomial distribution</h3>

<p>Binominal distribution =  yes or no answer. Only two outcomes F.E goal/miss, student/no student. Each outcomes is independent. Requires set probability. Assumptions when using the binomial distribution:<br>
• there are 2 outcomes to each trial (e.g. yes or no)<br>
• the probability of a yes (or a success) does not change from trial to trial (each trial is independent)  </p>

<p>If we generalize this situation we might define a binomial distribution as one where there are only two defined outcomes of any particular trial and the probability of each outcome remains constant from trial to trial (i.e. the events are independent). However, there are multiple ways to get the combination of events. Therefore we use:</p>

<p><strong>Combinations</strong><br>
N = number of trials<br>
R = number of successes<br>
<p align="left"> <img width="250" height="120" src="combinations.png"></p>

<p><strong>Factorials:</strong><br>
0! = 1<br>
1! = 1<br>
2! = 2 * 1 =    3<br>
3! = 3 * 2 * 1 = 6<br>
4! = 4 * 3 * 2 * 1 = 24<br>
etc.</p>

<p><p align="left"> <img width="500" height="200" src="binomial.png"></p>

<p><strong>Code</strong></p>

<ol>
<li>Binomial for a specific value for a certain sample. F.E. 2 from the sample are successful.</li>
<li>Binomial for a certain distribution of the sample. F.E. At most 2 in the sample are successful. Or 5 or more. </li>
<li>Binomial for a certain percentage of the  sample. F.E. 25% of the sample has x value or less. </li>
<li>Difference between two binomial values. F.E. Prob there are between 4 and 5 of the trials successful.</li>
</ol>

<div><pre><code class="language-none">1. dbinom(x, size = n, prob = y)    
2. pbinom(x, size = n, prob =y) 
3. qbinom(p, size = n, prob =y) 
4. diff(pbinom(c(X,Y), size = n, prob =y)   </code></pre></div>

<p><strong><em>Default = # or less (left area of the distribution). For # or more do: 1-probability of # or less</em></strong> </p>

<h3 id="toc_12">Poisson distribution</h3>

<p>Poisson distribution = defined by the average. Poisson works particularly well when =<br>
    - You can only count the characteristics but not without.<br>
    - There are a large number of trials but low number of probability.  </p>

<p><p align="left"> <img width="250" height="130" src="poisson.png"></p>

<p>x = number of occurrences<br>
L = average number of times a characteristics occurs<br>
e = constant   </p>

<p><strong>Expected value =</strong> n * p = LAMDA</p>

<ol>
<li>Poisson for a certain value. Lambda = n*p. F.E. Prob of having a 5</li>
<li>Poisson for a certain value distribution. F.E. Prob of having less than 5. More than 5? = 1- Ppois(4, lambda)</li>
<li>Poisson for a certain probability to capture a certain value. F.E. Poisson value for 25%.</li>
</ol>

<div><pre><code class="language-none">1. dpois(x,lambda)  
2. ppois(x, lambda) 
3. qpois(x,lambda)</code></pre></div>

<p><strong><em>Default = # or less (left area of the distribution). For # or more do: 1-probability of # or less</em></strong> </p>

<h3 id="toc_13">Examples different distributions:</h3>

<p><strong>(i) The number of goals scored during a football match?</strong><br>
Poisson distribution</p>

<p><strong>(ii) The numbers drawn in the UK lottery?</strong><br>
Uniform distribution</p>

<p><strong>(iii) The number of faulty items coming off a production line?</strong><br>
Binomial distribution</p>

<p><strong>(iv) The number of females attending a cinema for a particular film?</strong><br>
Poisson or binomial distribution</p>

<p><strong>(v) The number of people arriving at a queue?</strong><br>
Poisson distribution</p>

<div style="page-break-after: always;"></div>

<h2 id="toc_14">Topic 3 - The normal distribution</h2>

<p><strong>Normal distribution:</strong> a distribution that is symmetrical around its mean; To generalize, when a variable is continuous, and its value is affected by a large number of chance factors, none of which predominates.   </p>

<p><strong>Characteristics of a normal distribution:</strong><br>
    - Continuous (curve)<br>
    - Bell shaped - symmetrical<br>
    - Basis for statistical theory<br>
    - Distribution of the curve = 1<br>
    - Uni-model = single peak<br>
    - Described by mean and standard deviation<br>
    - Wider bell curve --&gt; larger normal distribution  </p>

<p>\(\mu\) = Mean<br>
\(\sigma\) = Standard deviation</p>

<p><p align="left"> <img width="150" height="80" src="zvalue.png"></p>

<p><strong>Empirical rule</strong></p>

<p>For all normal distributions: 68-95-99.7 rule</p>

<p>99.7% of observations are located between: -3 mu and 3<br>
95% of observations are located between: -2 mu 2<br>
68% of observations are located between: -1 mu 1</p>

<p><strong>Normal distribution</strong></p>

<p><strong>Z-value</strong></p>

<p>A standardized observation. Probabilities have been tabulated for the standard normal distribution. The z-value being the number of standard deviations away from the mean.</p>

<div><pre><code class="language-none">z &lt;- (x-mean)/sd</code></pre></div>

<ol>
<li>Normal distribution for a certain proportion. Pi = population proportion mean%.</li>
<li>Normal distribution for a certain value distribution. F.E. Prob of value above 5. FALSE
Prob less than 9. TRUE</li>
<li>Normal distribution for a certain probability to capture a certain value. F.E. Value that is given at 25% point. </li>
<li>Difference between two values on the normal distribution. F.E. between 5 and 10. </li>
</ol>

<div><pre><code class="language-none">1. pnorm(X, pi, sd, lower.tail = FALSE)
2. pnorm(X, mean = mean, sd = sd, lower.tail = FALSE)
3. qnorm(p, mean = mean, sd = sd, lower.tail = FALSE) 
4. diff(pnorm(c(X,Y), mean = mean, sd = sd, lower.tail = FALSE)</code></pre></div>

<p>lower.tail = TRUE: The area of the left side of the slope<br>
lower.tail = FALSE: The area of the right side of the slope</p>

<p><strong>Confidence interval for normal distribution</strong></p>

<div><pre><code class="language-none">z.test(x, sd=sigma)
binconf(x = x, n = n) &lt;- proportions
t.test(variable) &lt;- t-distribution for conf.inv</code></pre></div>

<h3 id="toc_15">Plotting the normal distribution</h3>

<p>&quot;With mean = 3 and standard deviation = 7<br>
Limits: mean +/- 3 * standard deviation  = 3*7 = 21 
Lower limit = 3 – 21 = -18<br>
Upper limit = 3 + 21 = 24&quot;  </p>

<p>Example:</p>

<div><pre><code class="language-none">x &lt;- seq(15, 45, length=50)
y &lt;- dnorm(x, 30, 5)
plot(x,y,type=&quot;l&quot;,lwd=2,col=&quot;black&quot;)

x &lt;- seq(15,35,length=100)
y &lt;- dnorm(x, 30,5 )
polygon(c(15,x,35),c(0,y,0), density = c(15, 35), col = &quot;black&quot;)

p &lt;- pnorm(35, mean = 30, sd = 5,lower.tail = TRUE)
text(0,0.15,&quot;68%&quot;)</code></pre></div>

<h3 id="toc_16">Binomial</h3>

<p>It will be possible to use the Normal distribution as an approximation to the Binomial if: 
n is large and p &gt; 0.1 </p>

<p>the mean = n*p<br>
standard deviation = √[np(1 − p)]<br>
X = given x +/- 0.5</p>

<p><p align="left"> <img width="220" height="80" src="np.png"></p>

<p><strong>Binomial for a sample</strong></p>

<ol>
<li>Density function (individual probability). </li>
<li>Cumulative density (between certain values). </li>
<li>Difference between two binomial values</li>
<li>Inverse cumulative density. For a certain prob. </li>
</ol>

<div><pre><code class="language-none">1. dbinom(x, mean, sd, lower.tail = FALSE)
2. pbinom(x, mean, sd, lower.tail = FALSE)
3. diff(pbinom(c(X,Y), mean = mean, sd = sd, lower.tail = FALSE)
4. qbinom(p, mean, sd, lower.tail = FALSE)</code></pre></div>

<h3 id="toc_17">Poisson</h3>

<p>Lambda = np = mean<br>
X = general x +/- 0.5</p>

<p><p align="left"> <img width=180" height="100" src="poissonfor.png"></p>

<p>This normal approximation can be used if lambda is &gt; 30</p>

<div style="page-break-after: always;"></div>

<h2 id="toc_18">Topic 4 - Samples, estimation &amp; confidence intervals</h2>

<p><strong>The standard error of the sampling distribution of the mean</strong></p>

<p>The sampling distribution of the sample mean will approximate to a normal distribution. The mean of the distribution is \(\mu\), the standard error \({e}\) = \(\sigma /\sqrt{n}\). That is the measure of spread for a sampling distribution, the standard deviation of the sampling distribution.</p>

<p>The point estimates gives us a particular value as an estimate of the population parameter. Usually for a normal distribution, the \(\bar{x}\) and the \(\mu\) are unbiased, meaning that when n tends to infinity, \(\bar{x}\) tends to \(\mu\) and s tends to \(\sigma\).</p>

<p>The confidence intervals gives us a range of values which is likely to contain the population parameter.</p>

<div><pre><code class="language-none">se &lt;- sigma / sqrt(n)</code></pre></div>

<p><strong>Probability sample</strong></p>

<ol>
<li>To find the probability that X is larger than mu</li>
<li>To find the probability that X is smaller than mu</li>
</ol>

<div><pre><code class="language-none">p &lt;- pnorm(X, mu, se, lower.tail = TRUE) 
p &lt;- pnorm(X, mu, se, lower.tail = FALSE)</code></pre></div>

<p><strong>Probability proportions sample</strong></p>

<div><pre><code class="language-none">sd &lt;- sqrt((pi*(n-pi))/n)
z &lt;- (p - pi)/sd

pnorm(X, pi, se, lower.tail =FALSE)</code></pre></div>

<p><strong>Sample size</strong></p>

<p>Package = &quot;samplingbook&quot;.</p>

<p>Provides the sample size needed to have a 95% confidence to estimate the population mean. 
Level = confidence level. Se is required standard error. </p>

<div><pre><code class="language-none">sample.size.mean(se, sigma, level=0.95)</code></pre></div>

<div style="page-break-after: always;"></div>

<h2 id="toc_19">Topic 5 - Significance testing</h2>

<p><strong>Significance testing</strong> aims to make statements about a population parameter or parameters on the basis of sample evidence called point estimates. Does the sample result support or are consistent with some fact or supposition about the population. If we want to infer a unknown population value, significance testing estimates a plausible interval for the population value. </p>

<p><strong>Parametric tests:</strong> Rely on assumptions about the shape of the underlying population distribution (normal distribution) and the parameters (means &amp; standard deviation) of the assumed distribution.</p>

<p><strong>Non-parametric tests:</strong> Rely on none or few assumptions about the form or parameters of the population distribution from which the sample was drawn. Includeds non-numeric values. </p>

<h3 id="toc_20">Hypothesis testing</h3>

<p><strong>Hypothesis testing:</strong> setting up an idea, concept or proposition we accept or reject. In term of structure we set up two hypothesis: Null hypothesis and Alternative hypothesis.  </p>

<p><strong>Null hypothesis</strong> = H0 = A statement of a no change or no difference from the idea or proposition we are testing. H0 is assumed to be true while we do the test. F.E. Innocent until proven guilty.<br>
    - H0 = \(\mu\) = \(\mu\)0 (mean).<br>
    - H0 = \(\pi\) = \(\pi\)0 for a %  </p>

<p><strong>The alternative hypothesis</strong> = HA = What we accept if on the basis of evidence (generally collected data) we can reject H0. Generally not specific. F.E. More than \(\mu\)<br>
    - H1 = \(\mu\) = different than u0<br>
    - H1 = \(\pi\) = different than \(\pi\)0  </p>

<p><p align="left"> <img width="450" height="350" src="hypothesis.png"></p>

<h3 id="toc_21">The difference between confidence interval and a test of significance</h3>

<p>Significance testing and confidence interval are related to each other. The confidence interval is explained as a set of acceptable hypotheses. The null hypothesis lies within the confidence interval when it is accepted whereas significance testing means using the sample data to find out if hypothesis are accepted or not. </p>

<p>Both, confidence intervals and significance testing, use sample data and rely on sampling distributions. They also affect each other as for example the sampling error becomes larger when the interval becomes wider by having a higher level of confidence, which means being more certain of a result. </p>

<p><strong>Confidence interval</strong> =  a set acceptable hypothesis. If we reject H0 we say we are confident that our results are not due to chance or sampling error.</p>

<p><strong>Test statistic =</strong>  Compares the sample evidence with the null hypothesis (value assumed true) approximated by the normal distributions. The closer to 0, the more likely H0 is correct.</p>

<ol>
<li>Values</li>
<li>Proportions</li>
</ol>

<p>z = X - \(\mu\) / se<br>
z = p - \(\pi\) / sqrt(\(\pi\)(100-\(\pi\)) / n )</p>

<p>P = Sample percentage<br>
Pie (\(\pi\)) = the claimed population percentage</p>

<p><strong>Finding confidence intervals:</strong></p>

<div><pre><code class="language-none">mu &lt;- mu
sd &lt;- sd
se &lt;- sd / (sqrt(n))
n &lt;- n
conf_int95 &lt;- cv * sd / (sqrt(n))
mu_plus &lt;- mu + conf_int95
mu_min &lt;- mu - conf_int95

ts &lt;- (X-U) / se</code></pre></div>

<h3 id="toc_22">Critical values</h3>

<p>Critical value = Defines the points at which the chance of the H0 being true is at a small, predetermined level. Significance level. Usually 5% --&gt; 95% confidence level --&gt; 1.96. If the test statistic is between -1.96 &amp; +1.96, we fail to reject H0. If it is more, it is an extreme value and we would reject H0 as it lies within the rejection area. </p>

<p>Two-sided test:</p>

<ul>
<li>H0 = \(\mu\) = \(\mu\)0 (mean). </li>
<li>HA = \(\mu\) = different then \(\mu\)</li>
</ul>

<p>Proportions:  </p>

<ul>
<li>H0 = \(\pi\) = \(\pi\)0 for a %<br></li>
<li>HA = \(\pi\) = different then \(\pi\)0</li>
</ul>

<p><strong>Critical value, sample &gt; 30</strong></p>

<ol>
<li>Two-sided: Critical value, 5% significance level = 1.96</li>
<li>Two-sided: Critical value, 1% significance level = 2.58</li>
<li>Two-sided: Critical value, 10% significance level = 1.96</li>
<li>One-sided: Critical value, 5% significance level = 1.64</li>
<li>One-sided: Critical value, 1% significance level = 2.33</li>
<li>One-sided: Critical value, 10% significance level = 1.28</li>
</ol>

<div><pre><code class="language-none">cv &lt;- qnorm(0.975)
cv &lt;- qnorm(0.995)
cv &lt;- qnorm(0.95)

cv &lt;- qnorm(0.95)
cv &lt;- qnorm(0.99)
cv &lt;- qnorm(0.90)</code></pre></div>

<p><strong>Critical values t-distribution</strong></p>

<ol>
<li>One-sided: critical value at a 5% significance level</li>
<li>One-sided: critical value at a 10% significance level</li>
<li>One-sided: critical value at a 1% significance level</li>
<li>Two-sided: critical value at a 5% significance level</li>
<li>Two-sided: critical value at a 10% significance level</li>
<li>Two-sided: critical value at a 1% significance level</li>
</ol>

<div><pre><code class="language-none">cv &lt;- qt(0.95, df) 
cv &lt;- qt(0.90, df) 
cv &lt;- qt(0.99, df)
cv &lt;- qt(0.975, df)
cv &lt;- qt(0.95, df) 
cv &lt;- qt(0.995, df)</code></pre></div>

<p>df = degree of freedom</p>

<p><strong>One-sided test</strong></p>

<p>We want to specify whether the real value is above or below the claimed value in those cases where we are able to reject the null hypothesis. Therefore, we want to concentrate the chance of rejecting H0 at one end of the normal distribution. The significance is 5%, the critical value -1.645 or +1.645. All 5% on one side. </p>

<h3 id="toc_23">Type of errors:</h3>

<p>There will always be some chance that the true population value really does lie outside of the confidence interval or that we will come to wrong decisions. For a 95% confidence level, there is a 5% chance you are wrong. </p>

<p><strong>Power of the test:</strong> being able to reject the hypothesis if it is not correct</p>

<p>Type 1 error: H0 is correct, but it is rejected<br>
Type 2 error: H0 is not correct, but it is accepted  </p>

<h3 id="toc_24">Sample significance testing</h3>

<p>Package: &quot;BSDA&quot;.</p>

<ol>
<li>Two-sided</li>
<li>One-sided: X is greater than the population mean</li>
<li>One-sided: X is less than the population mean</li>
</ol>

<div><pre><code class="language-none">1. tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;two.sided&quot;, var.equal = TRUE) 

2. tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;greater&quot;, var.equal = TRUE) 

3. tsum.test(mean.x = X, s.x = sd, n.x = n, mu = mu, alternative = &quot;less&quot;, var.equal = TRUE) </code></pre></div>

<p><strong>For proportions:</strong></p>

<div><pre><code class="language-none">prop.test(x= x,n = n,p = p,correct=TRUE,alternative=&quot;two.sided&quot;)</code></pre></div>

<p>Same goes for above: two.sided, greater, less  </p>

<h3 id="toc_25">Test of equality</h3>

<p>H0 &lt;- \(\mu1 = \mu2\) or \((\mu1 - \mu2) = 0\)<br>
HA &lt;- \(\mu1 \neq \mu2\) or \(\mu1 - \mu2 \neq 0\)</p>

<p>Difference in two means with a certain confidence level confidence interval. Default = 95%</p>

<div><pre><code class="language-none">tsum.test(mean.x = X, s.x = sd, n.x = n, mean.y = X, s.y = sd, n.y = n, var.equal=FALSE)</code></pre></div>

<p>2-sample test for equality of proportions without continuity correction.</p>

<div><pre><code class="language-none">data &lt;- matrix(c(values), byrow=TRUE, nrow=2)  
prop.test(data, correct=FALSE, alternative=&quot;greater&quot;)</code></pre></div>

<p>The usual value of the null hypothesis of the population parameter that we have as a difference with the sample estimation in the numerator of the test statistic in this case is equal to 0, since we postulating in the null hypothesis that the population percentages are equal so the difference is 0.</p>

<h3 id="toc_26">P-value</h3>

<p>P value = measure of risk that you reject the null hypothesis when you should not have. If you reject the null hypothesis, you have ..% chance that you made a mistake.</p>

<p>If p-value is &lt; then the significance value (5%), reject null hypothesis. 
P-value &gt; than the significance value, we cannot reject the null hypothesis. F.E. 1.96 critical value = significance value of 0.05. If you get a p-value of less than 0.05, we can reject the null hypothesis. </p>

<p><strong>Reservations about test conclusions</strong></p>

<ul>
<li>How was the efficiency measured?</li>
<li>How were the sample participants selected?</li>
<li>Were the conditions of the sample the same in each sample?</li>
<li>Did observing the sample affect performance?</li>
</ul>

<h2 id="toc_27">Topic 5 - Non-Parametric testing</h2>

<p>Looks at the overall distribution and compares this to some known or expected value. When do we use this? F.E. You only have nominalor ordinal data. Used when:<br>
    - A null hypothesis cannot be stated in terms of parameters<br>
    - A level of measurement has been achieved that gives validity to differences.<br>
    - The test statistics follows a known distribution</p>

<p>It is a statistical method that makes no assumption on the population distribution or sample size. In general, conclusions drawn from non-parametric methods are not as powerful as the parametric ones. However, as non-parametric methods make fewer assumptions, they are more flexible, more robust, and applicable to non-quantitative data.</p>

<p><strong>Contingency table / frequencies</strong></p>

<div><pre><code class="language-none">table(variable)
prob.table(variable</code></pre></div>

<h3 id="toc_28">Chi-square</h3>

<p>Chi-square test looks at whether there is a statistical association between the two sets of answers. May allow the development of a proposition that there is a causal link between the two. Steps:</p>

<ol>
<li>State the hypothesis:<br>
H0 there is no association between the two sets of answers<br>
HA there is an association between the two sets of answers</li>
<li>Choose statistical distribution. For cross-tabulations --&gt; chi-square distribution</li>
<li>State the significance level. Mostly 5%.</li>
<li>State critical value. </li>
<li>Calculate test statistic</li>
<li>Compare calculated value (test statistic) and the critical value </li>
</ol>

<p>Degree of freedom = # of row - 1 * # of columns = fixed</p>

<p>O = Observed cell frequencies (the actual answers)<br>
E = Expected cell frequencies (if the null hypothesis is true)</p>

<p>Expected value = probability of an independent event<br>
Expected cell frequency =  (row total) * (column total) / Grand total</p>

<p><p align="left"> <img width="600" height="300" src="chi.png"></p>

<p>The statistic calculated involves squaring values, and thus the result can only be positive. Shape is determined by the number of degrees of freedom.<br>
    - Whether a relationship exists within a cross tabulation<br>
    - The goodness of fit for a known distribution to a set of data</p>

<p>Relatively low df --&gt; positive skewed distribution<br>
Relatively high df --&gt; The shape of the distribution approaches normal distribution</p>

<ol>
<li>Chi-square test</li>
<li>Get the expected values</li>
<li>Probability for chi-square </li>
</ol>

<div><pre><code class="language-none">data &lt;- matrix(c(27,373,33,567),byrow=TRUE,nrow=2)
chisq.test(data,correct=FALSE)

chisq.test(data,correct=FALSE)$expected

prop.table(chisq.test(data,correct=FALSE)$expected,1)
prop.table(chisq.test(data,correct=FALSE)$expected,2)</code></pre></div>

<p><strong>All expected frequencies must be above five! If not, categories must be combined!</strong></p>

<p><strong>Conclusion:</strong></p>

<p>Chi-squared cannot be zero. If all expected cell frequencies were equal to the observed cell frequencies, then the value of chi-squared would be 0.</p>

<p>Any difference in observed - expected cell frequencies is either due to:<br>
    - Sampling error<br>
    - Association between answers  </p>

<ol>
<li>Difference more likely it is that there is an association<br></li>
<li>If calculated value &gt; than the critical value, we will fail to reject the H0. </li>
</ol>

<p><em>&quot;There appears to be an association between … and …. We need to examine how that association manifests itself, whether such an association is meaningful within the problem context and the extend to which the association can be explained by other factors. The association is likely to exist but not what it is or why it is.</em></p>

<h3 id="toc_29">Goodness of fit</h3>

<p><strong>Uniform:</strong></p>

<p>H0 = uniform distribution + There is no evidence that there is a difference between the numbers<br>
HA = not uniform distribution + There is evidence that there is a difference between the numbers  </p>

<p>Expected value = Total tasks completed / Number of operations  </p>

<p>Degree of freedom =  number of categories - number of parameters - 1.</p>

<div><pre><code class="language-none">x &lt;- c(frquencies)
p &lt;- c(rep(1/5,n))
chisq.test(x,p=p)</code></pre></div>

<p><strong>Binomial:</strong></p>

<p>Binomial -&gt; probability of success<br>
H0 = distribution is Binomial<br>
HA = distribution is not Binomial  </p>

<p>Expected value = Probability * value<br>
Degree of freedom = number of categories - number of parameters - 1.</p>

<p><p align="left"> <img width="500" height="230" src="chi.png"></p>

<p>Package &quot;actuar&quot;.</p>

<p>dbinom(x, size = n, prob = y)   </p>

<p>For example:</p>

<div><pre><code class="language-none">cj &lt;- c(-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5)

#or

cj &lt;- seq(from = -0.5, to=5, by=1)

nj &lt;- c(15,20,20,18,13,10)
data &lt;- grouped.data(Group = cj, Frequency = nj)
p &lt;- mean(data)/5
pr &lt;-c(dbinom(0,5,p),dbinom(1,5,p),dbinom(2,5,p),dbinom(3,5,p),dbinom(4,5,p),dbinom(5,5,p))

nj2 &lt;- c(35,20,18,23)
pr2 &lt;- c(dbinom(0,5,p)+dbinom(1,5,p),dbinom(2,5,p),dbinom(3,5,p),dbinom(4,5,p)+dbinom(5,5,p))

chisq.test(nj2,p=pr2)</code></pre></div>

<p><strong><em>All expected frequencies must be above five! If not, categories must be combined!</em></strong>  </p>

<p><strong>Poisson</strong></p>

<p>Poisson --&gt; mean / lambda<br>
H0 = distribution is Poisson<br>
HA = distribution is not Poisson   </p>

<p>Expected value = Probability * value<br>
Degree of freedom = number of categories - number of parameters - 1.</p>

<p>NOTE! Distribution goes to infinity. Counter for one value that is X or more. 1 - until X. </p>

<p>For example:</p>

<div><pre><code class="language-none">cj &lt;- c(-0.5, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5)

or

cj &lt;- seq(from = -0.5, to=6, by=1)
nj &lt;- c(16, 30, 37, 7, 10)
grouped.data(Group = cj, Frequency = nj)
m &lt;- mean(data)

pr &lt;- c(dpois(0, m),dpois(1,m),dpois(2, m), dpois(3, m), dpois(4, m), + (1-ppois(4,m)) )

chisq.test(nj, p = pr)</code></pre></div>

<h3 id="toc_30">Normal distribution</h3>

<p>Can involve more data manipulation since it will require grouped data and the calculation of two parameters (mean &amp; sd) before expected frequencies can be determined. </p>

<p>Degree of freedom = number of categories - number of parameters - 1<br>
Number of parameters = 2 --&gt; mean &amp; sd</p>

<ol>
<li>Transform into Z-values</li>
<li>Find probability for z value</li>
</ol>

<p>Expected frequency = prob * total value</p>

<p>For example:</p>

<div><pre><code class="language-none">cv &lt;- qchisq(0.90, 2)

cj &lt;- c(0, 1, 3, 10, 15, 30)
nj &lt;- c(16, 30, 37, 7, 10)
data &lt;- grouped.data(Group = cj, Frequency = nj) 
m &lt;- 6.2
s &lt;- 6.4

pr &lt;- c(pnorm(1,m,s), diff(pnorm(c(1,3),m,s)), diff(pnorm(c(3,10),m,s)), diff(pnorm(c(10,15),m,s)), 1 - pnorm(c(15),m,s) )
         
chisq.test(nj,p=pr)</code></pre></div>

<h3 id="toc_31">Mann-whitney test</h3>

<p>Non-parametric which deals with two samples that are independent and may be of different sizes. Equivalent of the t-test. Small samples &lt;10. We don&#39;t know if it is part of a normal distribution or where the level of measurement is at least ordinal. We are testing whether the positioning of values from the two samples in an ordered list follows the same pattern. </p>

<p>H0 = two samples come from the same population<br>
HA = two samples come from different populations  </p>

<p>N = Number of pairs - number of draws</p>

<p><strong>For small tests</strong></p>

<p><strong>Table of ranks =</strong> Difference between two scores and rank them by absolute size (ignore negative/positive). Ties are ignored.
When ranking and ties = 12.5 &amp; 12.5 f.e.</p>

<p>c1 values sample 1<br>
c2 values sample 2  </p>

<div><pre><code class="language-none">wilcox.text(c1,c2)</code></pre></div>

<p>Check the p-value or the result and compare with significance level of the distribution. </p>

<p><strong>Larger sample test &gt; 10</strong></p>

<p>You can use a approximation based on the normal distribution. Therefore critical values will be 1.96 for this two sided test. Here the p-value is 0.05. </p>

<h3 id="toc_32">Wilcoxon test</h3>

<p>Non-parametric equivalent of the t-test for matched pairs. Used to identify whether there is a change in behavior. Before &amp; after data. Here the basic premise is that while there will be changes in behavior, or opinions, the ranking of these changes will be random if there has been no overall change .</p>

<p>Small sample --&gt; specific critical value<br>
Large --&gt; normal distribution</p>

<p>Two options<br>
    - Do not predict direction --&gt; two sided<br>
    - Predict direction --&gt; one sided  </p>

<div><pre><code class="language-none">wilcox.test(w1, w2, paired=TRUE,correct=FALSE)</code></pre></div>

<p>Check p-value and test statistic to compare with the significance level.</p>

<h3 id="toc_33">Run test</h3>

<p>A test for randomness in a ‘yes/no’ type (dichotomized) variable, for example gender – either male or female, or behavioral like smoke or do not smoke. Includes nominal values.</p>

<p>Number of runs = The number of times in the sequence that we change in value.</p>

<p>Package &quot;randtests&quot;.</p>

<p>For example:</p>

<div><pre><code class="language-none">pers &lt;- c(0,1,1,0,0,0,0,1,1,0,1)
pers.f &lt;- factor(pers,labels=c(&quot;Male&quot;,&quot;Female&quot;))
runs.test(pers)</code></pre></div>

<h3 id="toc_34">P-value</h3>

<p>Find p value: Probability of getting this test statistic or more/less:</p>

<div><pre><code class="language-none">pchisq(ts, df, lower.tail=FALSE)</code></pre></div>

<div style="page-break-after: always;"></div>

<h2 id="toc_35">Topic 6 - Regressions, correlation and dummy&#39;s</h2>

<p>Correlation &amp; regression both deal with the analysis of the relationship between two or more variables. </p>

<p><strong>Correlation</strong> --&gt; the strength of the relationship<br>
<strong>Regression</strong> --&gt; describes the nature of that relationship between the variables by a model (or an equation)</p>

<div><pre><code class="language-none">model &lt;- lm(y~x, data = data)</code></pre></div>

<p><strong>Correlation:</strong> How two or more sets of observations are related. In a statistical context, we need to define correlation and establish a way of measuring it. Once we can do this we can try to decide what the result actually mean, assessing its significance in both statistical terms and in the context of the problem at which we are looking. </p>

<p><strong>Scatter diagram:</strong> Shows the two variables together, one on each axis. This is not a deterministic relationship and does not imply a cause effect between the two variables. </p>

<p>Dependent variable --&gt; y-axis<br>
Independent variable (explanatory) --&gt; x-axis</p>

<p><strong>Law of diminishing return: Non-linear.</strong>
The example has the law of diminishing returns. The law says that as more and more of one factor of production is used, with fixed amounts of the other factors, there will come a point when total output will fall. </p>

<p><strong>Deterministic linear relationship:</strong> 
Perfect relationship.</p>

<p><p align="left"> <img width="300" height="190" src="determenistic.png"></p>

<p>Positive vs negative linear relationship. </p>

<p><strong>Bivariate relationship.</strong>
Non-linear relationship. Makes no sense to calculate a linear relationship. </p>

<p><p align="left"> <img width="280" height="200" src="bivariate.png"></p>

<p><strong>Spurious correlation</strong><br>
We could be looking at two effects of a common cause and there will be no way of controlling, or predicting, the behavior of one variable by taking action on the other variable. F.E. two effects of a common cause would be ice cream sales and the grain harvest over time, both being affected by the weather. </p>

<p>Conditions, or variables, can be divided into two types, stable and differential, and it is the second type which is most likely to be identified as a cause. </p>

<p><strong>Correlation</strong></p>

<p>Next step to measure the strength of two associated variables. 
Coefficient correlations: measures the degree that the association between two variables is linear. Remember that the type of data will affect the way in which we interpret the answers calculated. Continuous - Ordinal data. </p>

<p>Correlation between two variables: cor(X,Y)<br>
Can be between -1 and 1. Negative when the slope is negative. Positive when the slop is positive. Closer to 1 means closer relationship. If it is 1, it is one straight line (linear). +1  means y increases in equal increments as x increases. If it is 0, there is no relationship.</p>

<div><pre><code class="language-{r">cor(data$X, data$Y)</code></pre></div>

<p><strong>Rank correlation / spearman</strong></p>

<p>Ordinal data (position of the data in an ordered list). Often applied where people or companies are asked to express preferences or to put a series if items into an order. Exercise caution when interpreting, since similar ranking of the same item may represent different views of the situation. </p>

<p><em>&quot;Are there similarities in the rankings of the products from one year to the next?’, or, ‘Is there an association between the rankings&quot;</em></p>

<p>Difference in rank (d) = last year - this year ranks</p>

<p>Rank correlation would not, normally, be used with continuous data since information would be lost by moving from actual measured values to simple ranks.</p>

<p><strong>Correlation for continuous data</strong></p>

<p>Non-linear. Does not rely on subjective judgement. More confidence in results. 
Results close to either -1 or +1 will indicate a high degree of association between two sets of data. </p>

<p><strong>Pearson&#39;s correlation coefficient</strong></p>

<p>The covariance of x and y (how much they vary together), divided by the root of the product of the variance of x and the variance of y (how much they each, individually, vary). </p>

<p>We need to decide the dependent variable (y) and the independent variable (x). As a general rule, we usually give the label y to the variable which we are trying to predict, or the one which we cannot control. </p>

<h3 id="toc_36">R-Squared</h3>

<p>Also called: Coefficient of the termination / measure of fitness. Only make sense for continuous data. Will always be positive 0-1 or 0-100%. </p>

<p>Interpretation: ..% of the variation in one of the variables is explained by the association between them. The rest is explained by other factors.<br>
    - The higher the r2, the more likely that the predictions are accurate.<br>
    - Values has been obtained from a specific set of data. Diff set might provide diff results<br>
    - Does not give evidence to cause and effect<br>
    - Explained only refers to explained by the analysis of variations.  </p>

<p>The R-squared measures the fraction of the fluctuation of the dependent variable related to fluctuations in the independent variables. What can be explained by the regression line. The R-squared is the ratio of variance. Variation is needed so you can use this to explain different factors. </p>

<p>Low r-squared does not mean that the regression is useless. It just means that there are other factors that we do not know affects the variable.</p>

<p><strong>Residuals:</strong> vertical distance of the line. What we cannot explain by the line. </p>

<p>Package: &quot;stargazer&quot;.</p>

<div><pre><code class="language-{r">Stargazer package =
stargazer(lm(Y~X, data=data), type=&quot;text&quot;)</code></pre></div>

<p><strong>Significance of correlation</strong></p>

<p>The statistical significance to determine whether correlation is affected by the number of observations. The &gt; observations, the lower the value of correlation has to be
&lt; observations, the higher</p>

<p>The correlation coefficient is treated as the sample value, r, and the population (or true) value of correlation is represented by the Greek letter r (rho). Distribution of the test statistic follows t-distribution. In most circumstance --&gt; one-tailed test. </p>

<p>H0: p = 0<br>
Ha: p &gt; 0</p>

<p><p align="left"> <img width="270" height="130" src="sigcor.png"></p>

<p>df = n-2 </p>

<h3 id="toc_37">Regressions</h3>

<p>Find the best line --&gt; the one that will give us the best prediction of y. How close the predictions of y are to the actual values of y. Simple regression model is give by:</p>

<p>Y = A + Bx</p>

<p>F.E. Price of a car (Y) vs income<br>
A = when there is 0 income, the price of the car would be only the constant (A).<br>
B = effect of income on the price of the car. For each 1000 more income, people pay B more for a car.  </p>

<p><strong>Plotting regression</strong></p>

<div><pre><code class="language-{r">plot(y~x,data=data, main=&quot;Title&quot;,ylab=&quot;Selling price&quot;,xlab=&quot;Size&quot;)</code></pre></div>

<p><strong>Residual error (y-Y) =</strong> The diff between what actually happened to y and what would be predicted if we use linear function. The large the error to respect to y, the greater the error we can expect when predicting y.</p>

<p><strong>Regression line:</strong></p>

<p>The line of best fit (least squares line). Minimizes the sum of squared differences of observed values from the regression line.The line which was derived from a desire to predict y values from x values (y on x);</p>

<p><p align="left"> <img width="300" height="200" src="reser.png"></p>

<p>D = vertical distance</p>

<p>For a straight line, y = a + bx, the value of A is the intercept on the y-axis when x = 0, so this value may be plotted. From the formula for calculating A we see that the line goes through the point (x , y ), which has already been calculated, so that this may be plotted. The two points are joined together.</p>

<div><pre><code class="language-{r">abline(lm(y~x,data=data),col=&quot;blue&quot;)</code></pre></div>

<p><strong>Creating the regression:</strong></p>

<ol>
<li>To plot the regression model</li>
<li>Summary regarding regression results</li>
<li>Evaluates the coefficient of the model</li>
<li>Only the first colum estimattion </li>
</ol>

<div><pre><code class="language-{r">model &lt;- lm(y~x, data = data)
summary(lm(Y~X,data=data))
summary(model)$coef
summary(model)$coef[,1]</code></pre></div>

<p><strong>Subsampling regression</strong></p>

<p>Estimate linear regression between two variables in two subsamples.
Specify dimentions [,]. First is row. Column, second. </p>

<ol>
<li>Selects the rows where age is larger than 45.</li>
<li>Lower than 45.</li>
</ol>

<div><pre><code class="language-{r">summary(lm(y-x, data=data[age&gt;=45,]))
summary(lm(y-x, data=data[age&lt;=45,]))</code></pre></div>

<p><strong>Confidence interval around slope</strong></p>

<div><pre><code class="language-{conf">confint(lm(variableY~variableX), level=0.95)</code></pre></div>

<p><strong>Confidence and predication bands  plotting</strong></p>

<p>Package: &quot;HH&quot;.  </p>

<div><pre><code class="language-{">fit &lt;- lm(variableY~variableX, data=data)
ci.plot(fit)</code></pre></div>

<p>Adds: observed values, fitted line, conf interval, predicted interval</p>

<h3 id="toc_38">Dummy variables, diff in means</h3>

<p>Dummy becomes the explanatory variable. The estimated coefficient is exactly the difference between the means of the two dummy variables. And the constant is the the average of dummy 0. P-value = equal to the p-value of the test of the difference in means. Needed when using categorical variables.   </p>

<p>For example, dummy:<br>
0 = Spanish<br>
1 = Non-Spanish  </p>

<p>Are there three variables?<br>
Rural: 1 if yes, 0, if not<br>
Urban: 1 if yes, 0, if not<br>
Suburban: 1 if yes, 0, if not  </p>

<p>Looking at the equation:<br>
meandummy0 + diffinmeansdummys * X</p>

<p>or</p>

<p>Y = B0 + B1X<br>
If x = 0, then y = B0<br>
If x = 1, then y = B0+B1<br>
B1 = the diff between the two dummy values. </p>

<p>For example:</p>

<div><pre><code class="language-{r">Y &lt;- c(Package$Pack1,Package$Pack2)
Package$dummy1 &lt;- 0
Package$dummy2 &lt;- 1
dummy &lt;- c(Package$dummy1,Package$dummy2)
newdat &lt;- data.frame(Allpack, dummy)

summary(lm(Y~dummy, data=newdat))</code></pre></div>

<h3 id="toc_39">Comparing regressions</h3>

<p>First we find the regression from each of the two variables. And then we can include a slope dummy.</p>

<p><strong>Slope dummy:</strong> takes the value zero in some rows and the value of a real independent variable elsewhere. Used when the data is not well modeled by a single straight line but fits two different straight lines. </p>

<p>Y = Constant0 + B0 * X - Diffinmeans + B1 * variable1*2</p>

<p>The multiple regression is able to duplicate the performance of the two simple regressions. But it can also do something that we could not accomplish with two simple regressions: test the significance of the difference between the two slopes. </p>

<p>For example:</p>

<div><pre><code class="language-{r">Time &lt;- c(Monterey$Time,Bakersfield$Time)
Boxes &lt;- c(Monterey$Boxes,Bakersfield$Boxes)
Monterey$dummy &lt;- 0
Bakersfield$dummy &lt;- 1
dummy &lt;- c(Monterey$dummy, Bakersfield$dummy)
Monterey$slopedummy &lt;- 0
Bakersfield$slopedummy &lt;- Bakersfield$Boxes
slopedummy &lt;- c(Monterey$slopedummy, Bakersfield$slopedummy)
newdat &lt;- data.frame(Time, Boxes, dummy, slopedummy)</code></pre></div>

<p><strong>Interpretation:</strong></p>

<p><p align="left"> <img width="350" height="200" src="interpretation.png"></p>

<p>Constant = mean of the included variable for Y. F.E. dummy 0. </p>

<p>Coefficient X = mean of x for the included variable. </p>

<p>Coefficient dummy = difference between the constant and mean of the excluded variable. F.E. difference between Spanish and non-Spanish for Y. </p>

<p>Coefficient slope dummy = difference between Y per X between the dummies. F.E. differnce between Spanish and non-spanish of Y per X. </p>

<p><strong>Ommiting the intercept:</strong></p>

<div><pre><code class="language-none">nfit &lt;- lm(var1 ~ var2 - 1, data)
Shows the means seperately and not the difference between means. Tests whether the expected counts are different from zero. </code></pre></div>

<p>Reorders group, to specific value to be first.</p>

<div><pre><code class="language-none">variable2 &lt;- relevel(variable, &quot;C&quot;)</code></pre></div>

<p><strong>Excluding the constant:</strong></p>

<p>-1 excludes the constant. Now we get the means of each variable seperatly. Not the difference in means. </p>

<div><pre><code class="language-none">summary(lm(Y~dummy1 + dummy2 - 1, data=newdata))</code></pre></div>

<div style="page-break-after: always;"></div>

<h2 id="toc_40">Topic 7 - Prediction</h2>

<p><strong>How do we estimate risk?</strong>  </p>

<p>Application to finance: Capital asset prediction model. The Capital Asset Pricing Model (CAPM) estimates β, which is a proportionality factor between the excess return of our project compared to the excess market return. To find the cost of capital r.  Then we apply this to find the NPV of a project.  </p>

<p><p align="left"> <img width="300" height="100" src="CAPM.png"></p>

<p>𝑟: rate of return of our project<br>
𝑟𝑓: risk free interest rate<br>
𝑟𝑚: market rate of return of similar projects<br>
β: parameter that can be estimated using regression analysis. The numerical measure of risk.</p>

<p>Everything else is known except r, the rate of return of the project it can be estimated using regression analysis.</p>

<p><p align="left"> <img width="300" height="100" src="CAPM2.png"></p>

<p>Our sample is subject to bias. Therefore, we need to construct a confidence interval around the prediction value.</p>

<p>CI = confidence interval<br>
PI = prediction interval </p>

<p>We need a confidence level around the slope, which will give us a confidence level around the rate of return of the invested capital, because the point estimate could be due to the randomness of the sample. </p>

<p>We make an estimation. Then check the confidence limits for predictions. If the confidence interval is very large, there is still a lot of risk. Don&#39;t go ahead with the project.</p>

<p>The confidence interval for prediction predicts a value for the dependent variable around the population line, while the confidence interval for the estimated mean predicts the point on the population line.</p>

<p>R square goodness of fit of the regression for coporate finance: The R-squared measures the fraction of the fluctuation of the dependent variable (Sunday Circulation) related to fluctuations in the independent variables (Daily Circulation). R squared is the ratio of variance due to market variance. Doesn&#39;t evaluate the usefulness of the regression.</p>

<h3 id="toc_41">Predictions</h3>

<p>Prediction is often broken down into two sections:<br>
    - Predictions from x values that are within the original range of x values: <strong>interpolation</strong><br>
    - Predictions from outside this range are called <strong>extrapolation.</strong>   </p>

<p>Interpolation is seen as relatively reliable. Extrapolation can be subject to unknown or unexpected effects (we are assuming that the linear regression will apply outside of the original range of x values). </p>

<p>Y = Dependent<br>
X = Explanatory</p>

<p>For example: 
We have the linear model equation: dist = -17.579 + 3.932X. To predict Y, we input X. </p>

<div><pre><code class="language-{r">xvalues &lt;- data.frame(variablename = c(1))
predict(model, newdata = xvalues)

or

predict(model, data.frame(Variablename = 1))</code></pre></div>

<p><strong>Confidence interval for prediction:</strong> </p>

<ol>
<li>One value</li>
<li>Multiple values from a existing data frame</li>
</ol>

<div><pre><code class="language-{confidence">predict(model, data.frame(Variablename = 100), interval = &quot;confidence&quot;, level=0.95)
predict(model, newdata = xvalues, interval = &quot;confidence&quot;, level=0.95)</code></pre></div>

<p>Output creates three values:
lower value (lwr), upper value (upr), fit the predicted value</p>

<p>This is a confidence interval for the mean predicted value, so there is a 95% chance that the mean distance will be within the given interval, and so there is a 5% chance that this interval misses the true average distance.</p>

<p><strong>Prediction interval for prediction:</strong></p>

<ol>
<li>One value</li>
<li>Multiple values from a existing data frame</li>
</ol>

<div><pre><code class="language-{r">predict(model, data.frame(Variablename = 100), interval = &quot;predict&quot;, level = 0.95)
predict(model, newdata = xvalues, interval = &quot;predict&quot;, level = 0.95)</code></pre></div>

<h3 id="toc_42">Prediction with dummy variables</h3>

<p>Equation:
Prediction = 𝛼1 + 𝛼2Constant<em>Dummy + 𝛽1𝑆𝑖𝑧𝑒 + 𝛽2</em>SlopeDummy</p>

<h3 id="toc_43">Prediction intervals example with dummies</h3>

<p><strong>Prediction</strong></p>

<div><pre><code class="language-none">fit &lt;- lm(Y ~ X + dummy + dummyslope, data=data)

predict(fit, data.frame(VariableX = c(10), Dummy = c(1),
Slopedummy = c(10)) )</code></pre></div>

<p><strong>Confidence interval prediction</strong>  </p>

<div><pre><code class="language-none">fit &lt;- lm(Y ~ X + dummy + dummyslope, data=data)

predict(fit, data.frame(VariableX = c(10), Dummy = c(1),
Slopedummy = c(10), interval=&quot;confidence&quot;)</code></pre></div>

<p><strong>Prediction interval</strong></p>

<div><pre><code class="language-none">fit &lt;- lm(Y ~ X + dummy + dummyslope, data=data)

predict(fit, data.frame(VariableX = c(10), Dummy = c(1),
Slopedummy = c(10), interval=&quot;predict&quot;)</code></pre></div>

<div style="page-break-after: always;"></div>

<h2 id="toc_44">Topic 8 - Data problems</h2>

<p><strong>Spurious correlation:</strong> occurs when the data coming from two unrelated variables is &quot;statistically&quot; correlated. There is no causality relationship. F.E. unemployment and snowfall.</p>

<p>There is a big difference between correlation and causation. Can be coincidental. Data dredging: taking analytics hoping to find relationships. Instead of having a problem and solving it. It is therefore important to think about what variables are sensible to use in a regression before running it. </p>

<p><strong>Quadratic:</strong></p>

<p>Y =  a + bX + Cx2</p>

<p><p align="left"> <img width="280" height="200" src="quadratic.png"></p>

<p>We can use model a quadratic relationship by adding a second independent variable to the regression. Here we add at the regression c = square of b. Then we look at the adjusted R-squared. </p>

<p><strong>Residuals</strong></p>

<p><strong>Fitted value =</strong> value of the dependent variable predicted by the regression model
<strong>Residual =</strong> difference between the observed value and the fitted value
<strong>Studentized residual =</strong> a residual divided by its standard error. If studentized residual is X this observation is X standard deviations away form zero. </p>

<p><strong>Residual plot</strong></p>

<div><pre><code class="language-none">m1 &lt;- lm(Y~X, data=data)
residual.plot(fitted(m1),resid(m1),sigma.hat(m1), main=&quot;Title&quot;)</code></pre></div>

<p><strong>Linear with an outlier</strong></p>

<p>The regression line is being pulled by the outlier:</p>

<p><p align="left"> <img width="280" height="200" src="regression.png"></p>

<p>Due NOT remove outliers from the data set unless they are due to a mistake. </p>

<p>If the residual is standardized, it will follow a standard normal distibution. An outlier in this distribution will fall outside a 95% confidence interval, which is defined by -1.96 and 1.96 (about -2 and 2), anything smaller than -2 or larger than 2 could be considered an outlier</p>

<p><strong>Influential observation</strong></p>

<p><strong>Influential observation:</strong> data point which has a large effect on the regression results. Does not necesarrily have to be an outlier. </p>

<p><p align="left"> <img width="280" height="200" src="influential.png"></p>

<p>A lot of &quot;leverage&quot; as it affects the regression equation. We can check whether it is an error, but we do not drop it from the regression. </p>

<p><strong>Influential measure test</strong></p>

<div><pre><code class="language-none">influence.measures(variable)</code></pre></div>

<h3 id="toc_45">Multicollinearity</h3>

<p>Multicollinearity is the term used to describe a high correlations amongst independent variables.</p>

<p>Therefore, we check the correlation between the two independent variables. Anything higher than 0.65 means there is a high correlation and there could be a multicollinearity problem.</p>

<p><strong>The variance inflation factor</strong> is an indicator of a multicollinearity problem. If the VIF is 10, there is a serious multicollinearity problem.</p>

<p>You can also eliminate one of the variables and experiment if there is a difference in the regression. </p>

<p>We should run a test of joint significance and we should not eliminate the variables if they are jointly significant.</p>

<p>If we run a test on the joint significance of the two variables, and they are not jointly significant, we can eliminate the variables from the regression.</p>

<p><strong>F-test:</strong></p>

<p>Allows us to test the joint significance of different regression coefficients.</p>

<p>Statistics, analysis of variance
Compare a base model that excludes the variables that could be significantly NON different than 0. If the p-value is low, we can reject the NULL hypothesis. Then they are significantly affecting each other. </p>

<p>H0: All coefficients included in the test are equal to 0.<br>
HA: If at least one coefficient is significantly different from 0, the null hypothesis should be rejected, but we cannot identify which coefficient is different from 0.  </p>

<p>If the p-value is small, then the null hypothesis can be rejected, so we have evidence that there is at least one coefficient which is significantly different from 0, and the coefficients should be not excluded from the regression.</p>

<ol>
<li>F-test</li>
<li>Variance inflation factors greater than 10</li>
</ol>

<div><pre><code class="language-none">anova(fit, fitres)
vif(fit)</code></pre></div>

<p><strong>Hidden</strong></p>

<p><strong>Extrapolation:</strong> predicting var away from the sample that we have<br>
<strong>Univariate statistics:</strong> min-max, range, skewness etc.<br>
<strong>Hidden extrapolation:</strong> X values are jointly far away from the data set. Gives very large prediction errors   </p>

<p>Extrapolation in regression means predicting far away from the sample and implies very unreliable predictions because of large prediction errors, it should not be done.</p>

<p><strong>Multicollinearity vs. Omitted variable bias:</strong></p>

<p><strong>Multicollinearity:</strong> creates a bias because of the variables that are included in the regression. </p>

<p><strong>Omitted variable:</strong> bias is caused by the variables left out of the regression. This can distort coefficients. It forces the variables that are present to carry the weight of both direct and indirect effects.</p>

<p><strong>Influence diagram:</strong> </p>

<p><p align="left"> <img width="350" height="200" src="influence.png"></p>

<p>Including irrelevant variables just reduces the precision of the estimates, while excluding relevant variables creates an omitted variables bias.</p>

<p>If the variables are individually or jointly significant, we should not exclude them from the regression.</p>

<h3 id="toc_46">ANOVA: Analysis of variance test.</h3>

<p><strong>Response variable:</strong> the variable we are comparing<br>
<strong>Factor variable:</strong> the categorical variable being used to define the groups. We will assume k samples (groups). </p>

<p>The one way is because each value is classified in exactly one way. Examples: race, color, gender etc.</p>

<p>H0 the means are all equal<br>
HA at least one of the means are different</p>

<p>The pairwise tests at the 5% level means that there is a 5% chance of rejecting the null hypothesis when it is true. This error adds up in the different t-tests increasing the probability of rejecting the null hypothesis when it is true. The ANOVA test instead is an &quot;omnibus&quot; test (it tests all the means at the same time) reducing the probability of rejecting the null hypothesis when it is true.</p>

<p><strong>The ANOVA test uses the F distribution!</strong></p>

<p><strong>One-way ANOVA:</strong> used to test the claim that three or more population means are equal. This is an extension to the two independent samples t-test.</p>

<div><pre><code class="language-none">res.aov &lt;- aov(Y ~ X, data = data)
summary(res.aov)</code></pre></div>

<p><strong>Two-way ANOVA:</strong> data falls into categories in two different ways: each observation can be placed in a table. F.e. Doctor and type of treatment</p>

<div><pre><code class="language-none">res.aov &lt;- aov(Y ~ X + X2, data = data)
summary(res.aov)</code></pre></div>

<p>With interaction</p>

<div><pre><code class="language-none">res.aov &lt;- aov(Y ~ X * X2, data = data)
summary(res.aov)</code></pre></div>

<p><strong>Three-way anova, MANOVA:</strong> more than two factors (generealized linear model)</p>

<ol>
<li>Test in difference</li>
<li>Test seperately</li>
</ol>

<div><pre><code class="language-none">test_manova &lt;- manova(cbind(Y, Y2) ~ X, data = data)
summary(test_manova)
summary.aov(test_manova)</code></pre></div>

<p><strong>Linear hypothesis test</strong></p>

<p>Example:</p>

<div><pre><code class="language-none">fit &lt;- lm(MKTDUB~pdub + poscar + pbpreg + pbpbeef,data=Hotdog)
linearHypothesis(fit,c(&quot;pbpreg + pbpbeef=0.0005&quot;), test=&quot;F&quot;)</code></pre></div>



<script type="text/javascript">
var _self="undefined"!=typeof window?window:"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?self:{},Prism=function(){var e=/\blang(?:uage)?-(\w+)\b/i,t=0,n=_self.Prism={util:{encode:function(e){return e instanceof a?new a(e.type,n.util.encode(e.content),e.alias):"Array"===n.util.type(e)?e.map(n.util.encode):e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/\u00a0/g," ")},type:function(e){return Object.prototype.toString.call(e).match(/\[object (\w+)\]/)[1]},objId:function(e){return e.__id||Object.defineProperty(e,"__id",{value:++t}),e.__id},clone:function(e){var t=n.util.type(e);switch(t){case"Object":var a={};for(var r in e)e.hasOwnProperty(r)&&(a[r]=n.util.clone(e[r]));return a;case"Array":return e.map&&e.map(function(e){return n.util.clone(e)})}return e}},languages:{extend:function(e,t){var a=n.util.clone(n.languages[e]);for(var r in t)a[r]=t[r];return a},insertBefore:function(e,t,a,r){r=r||n.languages;var l=r[e];if(2==arguments.length){a=arguments[1];for(var i in a)a.hasOwnProperty(i)&&(l[i]=a[i]);return l}var o={};for(var s in l)if(l.hasOwnProperty(s)){if(s==t)for(var i in a)a.hasOwnProperty(i)&&(o[i]=a[i]);o[s]=l[s]}return n.languages.DFS(n.languages,function(t,n){n===r[e]&&t!=e&&(this[t]=o)}),r[e]=o},DFS:function(e,t,a,r){r=r||{};for(var l in e)e.hasOwnProperty(l)&&(t.call(e,l,e[l],a||l),"Object"!==n.util.type(e[l])||r[n.util.objId(e[l])]?"Array"!==n.util.type(e[l])||r[n.util.objId(e[l])]||(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,l,r)):(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,null,r)))}},plugins:{},highlightAll:function(e,t){var a={callback:t,selector:'code[class*="language-"], [class*="language-"] code, code[class*="lang-"], [class*="lang-"] code'};n.hooks.run("before-highlightall",a);for(var r,l=a.elements||document.querySelectorAll(a.selector),i=0;r=l[i++];)n.highlightElement(r,e===!0,a.callback)},highlightElement:function(t,a,r){for(var l,i,o=t;o&&!e.test(o.className);)o=o.parentNode;o&&(l=(o.className.match(e)||[,""])[1],i=n.languages[l]),t.className=t.className.replace(e,"").replace(/\s+/g," ")+" language-"+l,o=t.parentNode,/pre/i.test(o.nodeName)&&(o.className=o.className.replace(e,"").replace(/\s+/g," ")+" language-"+l);var s=t.textContent,u={element:t,language:l,grammar:i,code:s};if(!s||!i)return n.hooks.run("complete",u),void 0;if(n.hooks.run("before-highlight",u),a&&_self.Worker){var c=new Worker(n.filename);c.onmessage=function(e){u.highlightedCode=e.data,n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(u.element),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},c.postMessage(JSON.stringify({language:u.language,code:u.code,immediateClose:!0}))}else u.highlightedCode=n.highlight(u.code,u.grammar,u.language),n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(t),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},highlight:function(e,t,r){var l=n.tokenize(e,t);return a.stringify(n.util.encode(l),r)},tokenize:function(e,t){var a=n.Token,r=[e],l=t.rest;if(l){for(var i in l)t[i]=l[i];delete t.rest}e:for(var i in t)if(t.hasOwnProperty(i)&&t[i]){var o=t[i];o="Array"===n.util.type(o)?o:[o];for(var s=0;s<o.length;++s){var u=o[s],c=u.inside,g=!!u.lookbehind,h=!!u.greedy,f=0,d=u.alias;u=u.pattern||u;for(var p=0;p<r.length;p++){var m=r[p];if(r.length>e.length)break e;if(!(m instanceof a)){u.lastIndex=0;var y=u.exec(m),v=1;if(!y&&h&&p!=r.length-1){var b=r[p+1].matchedStr||r[p+1],k=m+b;if(p<r.length-2&&(k+=r[p+2].matchedStr||r[p+2]),u.lastIndex=0,y=u.exec(k),!y)continue;var w=y.index+(g?y[1].length:0);if(w>=m.length)continue;var _=y.index+y[0].length,P=m.length+b.length;if(v=3,P>=_){if(r[p+1].greedy)continue;v=2,k=k.slice(0,P)}m=k}if(y){g&&(f=y[1].length);var w=y.index+f,y=y[0].slice(f),_=w+y.length,S=m.slice(0,w),O=m.slice(_),j=[p,v];S&&j.push(S);var A=new a(i,c?n.tokenize(y,c):y,d,y,h);j.push(A),O&&j.push(O),Array.prototype.splice.apply(r,j)}}}}}return r},hooks:{all:{},add:function(e,t){var a=n.hooks.all;a[e]=a[e]||[],a[e].push(t)},run:function(e,t){var a=n.hooks.all[e];if(a&&a.length)for(var r,l=0;r=a[l++];)r(t)}}},a=n.Token=function(e,t,n,a,r){this.type=e,this.content=t,this.alias=n,this.matchedStr=a||null,this.greedy=!!r};if(a.stringify=function(e,t,r){if("string"==typeof e)return e;if("Array"===n.util.type(e))return e.map(function(n){return a.stringify(n,t,e)}).join("");var l={type:e.type,content:a.stringify(e.content,t,r),tag:"span",classes:["token",e.type],attributes:{},language:t,parent:r};if("comment"==l.type&&(l.attributes.spellcheck="true"),e.alias){var i="Array"===n.util.type(e.alias)?e.alias:[e.alias];Array.prototype.push.apply(l.classes,i)}n.hooks.run("wrap",l);var o="";for(var s in l.attributes)o+=(o?" ":"")+s+'="'+(l.attributes[s]||"")+'"';return"<"+l.tag+' class="'+l.classes.join(" ")+'" '+o+">"+l.content+"</"+l.tag+">"},!_self.document)return _self.addEventListener?(_self.addEventListener("message",function(e){var t=JSON.parse(e.data),a=t.language,r=t.code,l=t.immediateClose;_self.postMessage(n.highlight(r,n.languages[a],a)),l&&_self.close()},!1),_self.Prism):_self.Prism;var r=document.currentScript||[].slice.call(document.getElementsByTagName("script")).pop();return r&&(n.filename=r.src,document.addEventListener&&!r.hasAttribute("data-manual")&&document.addEventListener("DOMContentLoaded",n.highlightAll)),_self.Prism}();"undefined"!=typeof module&&module.exports&&(module.exports=Prism),"undefined"!=typeof global&&(global.Prism=Prism);
</script>

<script type="text/x-mathjax-config">
(function () {

MathJax.Hub.Config({
	'showProcessingMessages': false,
	'messageStyle': 'none'
});

if (typeof MathJaxListener !== 'undefined') {
	MathJax.Hub.Register.StartupHook('End', function () {
		MathJaxListener.invokeCallbackForKey_('End');
	});
}

})();
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</body>

</html>
